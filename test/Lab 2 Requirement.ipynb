{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "291c7c1d",
   "metadata": {
    "id": "291c7c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 53, 34]\n"
     ]
    }
   ],
   "source": [
    "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
    "    return [actual_sorted_ids[id] for id in range(len(actual_sorted_ids)) if id < k]\n",
    "print(get_actual_ids_first_k([43,53,34,3,4,5],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222e84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "from typing import Dict, List, Annotated\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "class VecDB:\n",
    "\n",
    "    def __init__(self, file_path = \"PATH_DB_10K\", new_db = True) -> None:\n",
    "        self._file_path = file_path\n",
    "        if new_db:\n",
    "            # just open new file to delete the old one\n",
    "            #with open(self._file_path +\"/data_points.npy\", \"w\") as fout:\n",
    "                # if you need to add any head to the file\n",
    "                #pass\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
    "        #Features Vectors\n",
    "        embedings_rows=[]\n",
    "        self.db_size=len(rows)\n",
    "        for row in rows:\n",
    "            _, embed = row[\"id\"], row[\"embed\"]\n",
    "            embedings_rows.append(embed)\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(\"./\" + self._file_path , exist_ok=True)\n",
    "          \n",
    "        #Creating 1-Level Indexing File        \n",
    "        self._build_index(embedings_rows)\n",
    "        \n",
    "        \n",
    "    def retrive(self , query: Annotated[List[float], 5], top_k = 10):\n",
    "        # Approx. Worst Ram Usage for 20M data Points = 289.001 MB = 20M *(0.03% * (70features * 4Bytes + 4Bytes + 2* (2 score tuple * 4Bytes ))) + 10K * ( 70features * 4Bytes + 2* (2 score tuple * 4Bytes )+ (2 index tuple * 4Bytes ) ) = 303.04 * 10^6 Bytes        \n",
    "        # Loading centroids do use it in calculating cosine simularity with query vector\n",
    "        centriods = np.load(\"./\" + self._file_path + \"/Cluster_Centriods.npy\",allow_pickle=True)\n",
    "        #print(\"centriods Dimention is : \", len(centriods),\" centriod of \",len(centriods[0]),\" features.\")\n",
    "        scores = []\n",
    "        for id in range(len(centriods)):\n",
    "            # Access the current centroid using its index\n",
    "            centriod = centriods[id]\n",
    "            #calculating cosine simularity\n",
    "            score = self._cal_score(query, centriod)\n",
    "            scores.append((score, id))\n",
    "        sorted_scores= sorted(scores, reverse=True)            \n",
    "        # here we assume that if two rows have the same score, return the lowest ID\n",
    "        first_index = sorted_scores[0][1] #First closest Centriod index\n",
    "        ########################TODO : comment this code\n",
    "        ##print ('First closest Centriod is : ', first_index)\n",
    "        ########################\n",
    "        # Loading centroids ids points use it in calculating cosine simularity with query vector\n",
    "        second_index = sorted_scores[1][1] #Second closest Centriod index\n",
    "        ########################TODO : comment this code\n",
    "        #print ('Second closest Centriod is : ', second_index)\n",
    "        ########################\n",
    "        # Loading centroids ids points use it in calculating cosine simularity with query vector\n",
    "        third_index = sorted_scores[2][1] #Third closest Centriod index\n",
    "        ########################TODO : comment this code\n",
    "        #print ('Third closest Centriod is : ', third_index)\n",
    "        ########################\n",
    "        # Loading centroids ids points use it in calculating cosine simularity with query vector                \n",
    "        cluster_indexes = np.load(\"./\" + self._file_path + \"/Cluster_Indexes.npy\",allow_pickle=True)\n",
    "        \n",
    "        first_cluster_start_index = cluster_indexes[first_index][0]\n",
    "        first_cluster_end_index = cluster_indexes[first_index][1]\n",
    "        #print(\"first_cluster_start_index is : \",first_cluster_start_index,\" & first_cluster_end_index \",first_cluster_end_index)\n",
    "        \n",
    "        second_cluster_start_index = cluster_indexes[second_index][0]\n",
    "        second_cluster_end_index = cluster_indexes[second_index][1]\n",
    "        #print(\"second_cluster_start_index is : \",second_cluster_start_index,\" & second_cluster_end_index \",second_cluster_end_index)\n",
    "        \n",
    "        third_cluster_start_index = cluster_indexes[third_index][0]\n",
    "        third_cluster_end_index = cluster_indexes[third_index][1]\n",
    "        #print(\"third_cluster_start_index is : \",third_cluster_start_index,\" & third_cluster_end_index \",third_cluster_end_index)\n",
    "        \n",
    "        # Loading chosen points indexes use it in calculating cosine simularity with query vector        \n",
    "        indexes =[]\n",
    "        data_points_indexes = np.lib.format.open_memmap(\"./\" + self._file_path + \"/data_points_indexes\"+\".npy\", mode='r', dtype='int') \n",
    "\n",
    "        first_cluster_data_points_indexes = data_points_indexes[first_cluster_start_index:first_cluster_end_index]\n",
    "        indexes += first_cluster_data_points_indexes.tolist()\n",
    "        \n",
    "        second_cluster_data_points_indexes = data_points_indexes[second_cluster_start_index:second_cluster_end_index]\n",
    "        indexes += second_cluster_data_points_indexes.tolist()   \n",
    "        \n",
    "        third_cluster_data_points_indexes = data_points_indexes[third_cluster_start_index:third_cluster_end_index]\n",
    "        indexes += third_cluster_data_points_indexes.tolist() \n",
    "        \n",
    "        #print(\"indexes (First 10) = \", indexes[:10])\n",
    "        #print(\"indexes Dimention is : \", len(indexes),\" index\")\n",
    "        \n",
    "        # Loading chosen points use it in calculating cosine simularity with query vector\n",
    "        \n",
    "        rows =[]\n",
    "        data_points = np.lib.format.open_memmap(\"./\" + self._file_path + \"/data_points\"+\".npy\", mode='r', dtype='float32' , shape=(len(indexes),70)) \n",
    "\n",
    "        first_cluster_data_points = data_points[first_cluster_start_index:first_cluster_end_index]\n",
    "        rows += first_cluster_data_points.tolist() \n",
    "        \n",
    "        second_cluster_data_points = data_points[second_cluster_start_index:second_cluster_end_index]\n",
    "        rows += second_cluster_data_points.tolist()   \n",
    "        \n",
    "        third_cluster_data_points = data_points[third_cluster_start_index:third_cluster_end_index]\n",
    "        rows += third_cluster_data_points.tolist()           \n",
    "        \n",
    "        #print(\"sorted rows Data Dimention is : \", len(rows),\" point of \",len(rows[0]),\" features.\")\n",
    "        #print(\"sorted rows Data (First 10 row)(First 3 feature) = \", [row[:3] for row in rows[:10]])\n",
    "\n",
    "        final_scores = []\n",
    "        for i in range(len(rows)):\n",
    "            #calculating cosine simularity\n",
    "            score = self._cal_score(query, rows[i])\n",
    "            final_scores.append((score, indexes[i]))\n",
    "\n",
    "        final_indexes = sorted(final_scores  , reverse=True)[:top_k] #closest points\n",
    "        ########################TODO : comment this code\n",
    "        #print ('closest Points_indexes is : ', [s[1] for s in final_indexes])\n",
    "        #print ('closest Points score is : ', [s[0] for s in final_indexes])\n",
    "        ########################            \n",
    "        return [s[1] for s in final_indexes]        \n",
    "\n",
    "\n",
    "    # Calculate Cosine Similarity \n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "\n",
    "     \n",
    "     \n",
    "\n",
    "    def _build_index(self,rows: List[Annotated[List[float], 70]]):\n",
    "        #Approx. Worst Ram Usage for 20M data Points = 10.657 GB = 20M *(70features * 4Bytes + 70features * 4Bytes + 4Bytes + 4Bytes + 4Bytes) + 10K * ( 70features * 4Bytes + 2indexes * 4Bytes + 4Bytes ) = 1.144292 * 10^10 Bytes\n",
    "        #print(\"rows_points Dimention is : \", len(rows),\" point of \",len(rows[0]),\" features.\")\n",
    "        #print(\"First 10 row_points (First 3 feature): \", [row[:3] for row in rows[:10]])\n",
    "\n",
    "\n",
    "        # num_Clusters = Data Size / 2000 #Every Cluster has 2000 record \n",
    "        # batch_Size = By sense\n",
    "        \n",
    "        batch_Size=100\n",
    "        n_Clusters=10\n",
    "        if len(rows)== 10000: #10K\n",
    "            batch_Size= 2000\n",
    "            n_Clusters= 10\n",
    "        elif len(rows)== 100000: #100K\n",
    "            batch_Size= 20000\n",
    "            n_Clusters= 50\n",
    "        elif len(rows)== 1000000: #1M\n",
    "            batch_Size= 100000\n",
    "            n_Clusters= 500\n",
    "        elif len(rows)== 5000000: #5M\n",
    "            batch_Size= 1000000\n",
    "            n_Clusters= 2500  \n",
    "        elif len(rows)== 10000000:#10M\n",
    "            batch_Size= 1000000\n",
    "            n_Clusters= 5000\n",
    "        elif len(rows)== 15000000: #15M\n",
    "            batch_Size= 1000000\n",
    "            n_Clusters= 7500 \n",
    "        elif len(rows)== 20000000: #20M\n",
    "            batch_Size= 1000000\n",
    "            n_Clusters= 10000 \n",
    "            \n",
    "        batch_begin = 0\n",
    "        ##Begin of learning Phase\n",
    "        kmeans = MiniBatchKMeans(n_clusters = n_Clusters, batch_size = batch_Size, n_init=1)\n",
    "        counter = len(rows)//batch_Size -1 # For Loaping over whole Data \n",
    "        \n",
    "        while counter >= 0: #Loaping over whole Data using small batches steps \n",
    "            #Update k means estimate on a single mini-batch X.\n",
    "            kmeans = kmeans.partial_fit(rows[batch_begin:batch_begin+batch_Size])\n",
    "            batch_begin+=batch_Size\n",
    "            counter-=1\n",
    "            \n",
    "        #Compute cluster centers and predict cluster index for each sample.    \n",
    "        labels = kmeans.predict(rows)\n",
    "        #Rearrange Data samples in List of Centiode each one has it's own points ids.\n",
    "        arranged_data_samples = [[] for _ in range(n_Clusters)]\n",
    "        #To git size of each Cluster\n",
    "        clusters_size = [0 for _ in range(n_Clusters)]\n",
    "        for index in range(len(rows)):\n",
    "                arranged_data_samples[labels[index]].append(index)\n",
    "                clusters_size[labels[index]] += 1 # increamnt\n",
    "        #for i in range(n_Clusters):        \n",
    "            ##print(\"Cluster [\",i,\"] : size of points: \", clusters_size[i])\n",
    "            ##print(\"Cluster [\",i,\"] : First 5 points ids: \", arranged_data_samples[i][:5])\n",
    "\n",
    "        #ReSort Data samples in List of indexes based on Clusters's index arrange from centiod 0 to #n_Clusters each one has it's own points ids.\n",
    "        sorted_data_sample_indexes =[]  \n",
    "        sorted_data_sample =[]  \n",
    "        for cluster in arranged_data_samples:\n",
    "                for index in cluster:\n",
    "                    sorted_data_sample.append(rows[index])   #List of Float Embeddings 70 Feature Vectors                     \n",
    "                    sorted_data_sample_indexes.append(index) #List of Int IDs      \n",
    "        #print(\"First 10 sorted points (First 3 feature): \", [row[:3] for row in sorted_data_sample[:10]])\n",
    "        #print(\"First 10 sorted points ids: \", sorted_data_sample_indexes[:10])                    \n",
    "        clusters_start_end_indexes = [[0,0] for _ in range(n_Clusters)] # To read Embeddings 70 Feature Vectors List\n",
    "        start_index = 0 #num_elemnts\n",
    "        end_index = 0 #num_elemnts\n",
    "        #Loop\n",
    "        for i in range(n_Clusters):\n",
    "            end_index = start_index  + clusters_size[i] \n",
    "            clusters_start_end_indexes[i] = [start_index,end_index] \n",
    "            start_index = end_index\n",
    "        #for i in range(n_Clusters):       \n",
    "            #print(\"Cluster [\",i,\"] : start_index : \",clusters_start_end_indexes[i][0],\" & end_index \",clusters_start_end_indexes[i][1])\n",
    "                \n",
    "        \n",
    "        #Saving for loading it in retrive func. \n",
    "        np.save(\"./\" + self._file_path + \"/data_points\", sorted_data_sample)  \n",
    "        #Saving for loading it in retrive func. \n",
    "        np.save(\"./\" + self._file_path + \"/data_points_indexes\", sorted_data_sample_indexes)         \n",
    "        #Saving Data samples Centiodes for loading it in retrive func.\n",
    "        np.save(\"./\" + self._file_path + \"/Cluster_Centriods\", kmeans.cluster_centers_ )\n",
    "        #Saving Start & End indexes in each Centiode for loading it in retrive func.\n",
    "        np.save(\"./\" + self._file_path + \"/Cluster_Indexes\", clusters_start_end_indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b3e37",
   "metadata": {},
   "source": [
    "## These are the functions for running and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0ec8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from vec_db import VecDB\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "#from memory_profiler import memory_usage\n",
    "import gc\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    run_time: float\n",
    "    top_k: int\n",
    "    db_ids: List[int]\n",
    "    actual_ids: List[int]\n",
    "\n",
    "results = []\n",
    "to_print_arr = []\n",
    "\n",
    "def run_queries(db, query, top_k, actual_ids, num_runs):\n",
    "    global results\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        tic = time.time()\n",
    "        db_ids = db.retrive(query, top_k)\n",
    "        toc = time.time()\n",
    "        run_time = toc - tic\n",
    "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
    "    return results\n",
    "\n",
    "def memory_usage_run_queries(args):\n",
    "    global results\n",
    "    # This part is added to calcauate the RAM usage\n",
    "    #mem_before = max(memory_usage())\n",
    "    #mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
    "    return results, 5 #max(mem) - mem_before\n",
    "\n",
    "def evaluate_result(results: List[Result]):\n",
    "    # scores are negative. So getting 0 is the best score.\n",
    "    scores = []\n",
    "    run_time = []\n",
    "    for res in results:\n",
    "        run_time.append(res.run_time)\n",
    "        # case for retireving number not equal to top_k, socre will be the lowest\n",
    "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
    "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
    "            continue\n",
    "        score = 0\n",
    "        for id in res.db_ids:\n",
    "            try:\n",
    "                ind = res.actual_ids.index(id)\n",
    "                if ind > res.top_k * 3:\n",
    "                    score -= ind\n",
    "            except:\n",
    "                score -= len(res.actual_ids)\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
    "\n",
    "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
    "    return [id for id in actual_sorted_ids if id < k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fc6d6",
   "metadata": {},
   "source": [
    "## This to generate 10K database and the query using the seed numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9818bc3c",
   "metadata": {
    "id": "9818bc3c"
   },
   "outputs": [],
   "source": [
    "QUERY_SEED_NUMBER = 10\n",
    "DB_SEED_NUMBER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a159a5",
   "metadata": {
    "id": "fa3f0456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query are :  [[0.7765375  0.9560017  0.2640193  0.20768178 0.79258186 0.82844484\n",
      "  0.51472414 0.1492821  0.8328704  0.51280457 0.15334606 0.13591957\n",
      "  0.41092372 0.6890364  0.4036622  0.8417477  0.00812364 0.42550898\n",
      "  0.52419096 0.956926   0.23533827 0.8253329  0.07183987 0.3382153\n",
      "  0.74872607 0.57576054 0.93872505 0.75330186 0.9143402  0.8271039\n",
      "  0.1357916  0.9334384  0.8445934  0.14499468 0.9784896  0.7455802\n",
      "  0.31431204 0.13935137 0.3885808  0.9065287  0.78565687 0.22611439\n",
      "  0.49179822 0.8532397  0.64099747 0.3063178  0.11379027 0.96983033\n",
      "  0.2343331  0.5178342  0.6922639  0.32247454 0.5165536  0.2824335\n",
      "  0.8366852  0.60586494 0.17676568 0.33376443 0.68798494 0.67864877\n",
      "  0.31203574 0.15442502 0.14845031 0.24977547 0.7895685  0.8698942\n",
      "  0.3430732  0.6003678  0.49958014 0.26198304]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(DB_SEED_NUMBER)\n",
    "vectors = rng.random((10000, 70), dtype=np.float32)\n",
    "#print ('vectors are : ',vectors)\n",
    "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
    "query = rng.random((1, 70), dtype=np.float32)\n",
    "print ('query are : ',query)\n",
    "actual_sorted_ids_10k = np.argsort(vectors.dot(query.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27df97",
   "metadata": {},
   "source": [
    "## Open new DB add 10K then retrieve and evaluate. Then add another 90K (total 100K) then retrieve and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0a2d337",
   "metadata": {
    "id": "b0a2d337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_sorted_ids is :  [6233, 3332, 2038, 6587, 8367, 1361, 7600, 9925, 5065, 9640]\n",
      "actual_sorted_points score is :  [array([0.8718706], dtype=float32), array([0.8687765], dtype=float32), array([0.8622327], dtype=float32), array([0.8602855], dtype=float32), array([0.8589762], dtype=float32), array([0.8583203], dtype=float32), array([0.8576241], dtype=float32), array([0.8573488], dtype=float32), array([0.85469675], dtype=float32), array([0.8544617], dtype=float32)]\n",
      "10K\tscore\t0.0\ttime\t0.13\tRAM\t5.00 MB\n"
     ]
    }
   ],
   "source": [
    "top_k =10\n",
    "db = VecDB(file_path = 'PATH_DB_10K', new_db = False)\n",
    "records_dict = [{\"id\": i, \"embed\": list(row)} for i, row in enumerate(vectors)]\n",
    "db.insert_records(records_dict)\n",
    "res = run_queries(db, query, 10 , actual_sorted_ids_10k, 1) # one run to make everything fresh and loaded\n",
    "########################TODO : comment this code\n",
    "final_scores=[]\n",
    "actual_sorted_topk= [actual_sorted_ids_1k[id] for id in range(len(actual_sorted_ids_1k)) if id < 10]\n",
    "print ('actual_sorted_ids is : ', actual_sorted_topk)\n",
    "for id in range(len(actual_sorted_topk)):\n",
    "            # Access the current centroid using its index\n",
    "            point_id = actual_sorted_topk[id]\n",
    "            #calculating cosine simularity\n",
    "            score = db._cal_score(query, vectors[point_id])\n",
    "            final_scores.append(score)\n",
    "print ('actual_sorted_points score is : ', final_scores)\n",
    "########################\n",
    "res, mem = memory_usage_run_queries((db, query, 10, actual_sorted_ids_10k, 5)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"10K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f46790",
   "metadata": {
    "id": "83a2cf4c"
   },
   "source": [
    "## Remove exsiting varaibles to empty some RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6148c80",
   "metadata": {
    "id": "d6148c80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vectors\n",
    "del query\n",
    "del actual_sorted_ids_10k\n",
    "del records_dict\n",
    "del db\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cafc66",
   "metadata": {
    "id": "1d0f0f04"
   },
   "source": [
    "## This code to generate 20M database. The seed (50) will not be changed. Create the same DB and prepare it's files indexes and every related file.\n",
    "Note at the submission I'll not run the insert records.\n",
    "The query istelf will be changed at submissions day but not the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503faa7",
   "metadata": {
    "id": "48f2a175"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc61691",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_20M = np.load(\"./PATH_DB_1K/data_points.npy\",allow_pickle=True)\n",
    "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
    "query = rng.random((1, 70), dtype=np.float32)\n",
    "\n",
    "actual_sorted_ids_20m = np.argsort(vectors_20M.dot(query.T).T / (np.linalg.norm(vectors_20M, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0664c6",
   "metadata": {
    "id": "6e0664c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_sorted_ids is :  [664, 827, 458, 515, 535, 52, 483, 377, 855, 513, 795, 719, 822, 467, 581, 141, 730, 892, 994, 529, 405, 115, 934, 534, 488, 251, 91, 292, 619, 58, 555, 840, 159, 713, 217, 110, 13, 891, 376, 163, 202, 392, 143, 209, 743, 519, 576, 952, 375, 194, 224, 321, 707, 709, 423, 717, 306, 37, 51, 353, 894, 554, 567, 104, 318, 290, 308, 6, 484, 955, 670, 566, 965, 939, 204, 157, 736, 28, 927, 383, 245, 784, 259, 114, 26, 954, 695, 856, 942, 356, 388, 191, 816, 541, 689, 112, 814, 476, 510, 492, 285, 489, 958, 165, 626, 585, 734, 629, 304, 959, 548, 378, 505, 319, 578, 770, 485, 203, 428, 417, 787, 910, 925, 986, 373, 681, 539, 120, 477, 727, 531, 545, 956, 179, 244, 838, 466, 479, 399, 917, 989, 686, 269, 355, 270, 831, 644, 186, 690, 741, 228, 365, 731, 382, 800, 460, 845, 507, 449, 516, 268, 389, 973, 39, 282, 338, 599, 733, 915, 782, 164, 288, 41, 500, 137, 339, 899, 597, 662, 832, 151, 456, 705, 763, 133, 808, 860, 553, 693, 841, 183, 27, 761, 604, 936, 187, 879, 825, 798, 659, 100, 158, 941, 810, 887, 432, 518, 77, 349, 147, 815, 340, 221, 70, 747, 790, 779, 86, 595, 611, 71, 774, 881, 698, 325, 968, 716, 219, 673, 818, 846, 234, 320, 848, 144, 906, 983, 970, 558, 153, 314, 938, 72, 493, 390, 166, 990, 537, 24, 688, 248, 632, 300, 961, 921, 944, 33, 890, 646, 363, 229, 413, 870, 996, 329, 549, 55, 255, 88, 639, 867, 298, 169, 700, 547, 450, 863, 498, 362, 589, 334, 134, 799, 813, 656, 882, 630, 57, 99, 446, 397, 195, 624, 475, 701, 272, 50, 758, 197, 391, 412, 806, 872, 84, 777, 764, 154, 748, 883, 127, 343, 679, 438, 628, 605, 674, 408, 316, 502, 718, 188, 415, 189, 369, 481, 487, 148, 374, 126, 678, 506, 864, 128, 264, 5, 655, 256, 111, 18, 756, 275, 135, 238, 527, 173, 722, 414, 119, 583, 448, 85, 73, 53, 675, 723, 402, 750, 146, 666, 692, 742, 712, 45, 834, 744, 35, 293, 819, 538, 494, 988, 220, 737, 305, 116, 445, 465, 32, 287, 136, 976, 603, 254, 928, 732, 379, 971, 286, 653, 266, 999, 967, 792, 738, 772, 766, 440, 574, 912, 760, 132, 643, 853, 512, 98, 258, 811, 469, 80, 352, 943, 724, 354, 253, 691, 608, 176, 103, 291, 92, 903, 303, 847, 729, 751, 625, 124, 767, 172, 702, 560, 283, 590, 61, 573, 406, 30, 842, 621, 38, 225, 658, 262, 508, 400, 588, 694, 447, 797, 780, 880, 807, 648, 975, 926, 725, 528, 139, 21, 998, 706, 641, 236, 76, 215, 929, 471, 240, 395, 125, 914, 23, 714, 177, 711, 564, 612, 897, 610, 820, 235, 496, 726, 274, 213, 809, 350, 616, 835, 398, 333, 181, 49, 972, 888, 602, 985, 480, 514, 940, 524, 233, 504, 526, 317, 757, 642, 210, 89, 523, 122, 443, 579, 704, 299, 785, 762, 769, 78, 594, 665, 486, 138, 522, 923, 788, 79, 584, 96, 223, 170, 533, 315, 182, 960, 682, 572, 499, 613, 396, 587, 357, 930, 211, 593, 430, 175, 569, 368, 381, 222, 324, 977, 401, 544, 905, 431, 843, 540, 776, 427, 409, 65, 201, 150, 346, 978, 877, 669, 62, 951, 735, 794, 242, 93, 676, 775, 697, 459, 328, 462, 783, 874, 920, 823, 781, 342, 311, 94, 517, 34, 991, 145, 278, 457, 592, 435, 384, 636, 113, 473, 803, 607, 205, 117, 932, 649, 791, 200, 410, 746, 907, 771, 680, 232, 878, 948, 482, 90, 10, 836, 289, 980, 105, 546, 511, 199, 478, 993, 243, 109, 184, 551, 966, 81, 804, 273, 913, 568, 208, 11, 617, 885, 185, 851, 140, 284, 231, 858, 3, 347, 250, 957, 623, 294, 257, 174, 715, 924, 312, 106, 168, 171, 850, 64, 261, 364, 866, 180, 156, 618, 281, 442, 945, 142, 844, 265, 360, 280, 953, 162, 768, 1, 56, 647, 786, 949, 9, 192, 491, 950, 453, 433, 634, 337, 101, 472, 638, 609, 297, 562, 962, 14, 237, 663, 509, 873, 178, 916, 600, 802, 118, 525, 937, 898, 586, 992, 247, 536, 728, 660, 371, 361, 452, 87, 82, 801, 249, 241, 708, 296, 979, 640, 330, 44, 411, 931, 495, 277, 474, 667, 857, 468, 12, 370, 246, 946, 778, 501, 570, 821, 196, 326, 789, 935, 969, 43, 901, 212, 793, 155, 869, 911, 552, 615, 48, 661, 344, 394, 335, 47, 671, 444, 520, 889, 367, 854, 918, 740, 637, 387, 331, 279, 407, 441, 652, 22, 307, 434, 852, 436, 322, 745, 359, 358, 190, 896, 886, 651, 2, 532, 503, 685, 796, 54, 919, 380, 839, 227, 437, 871, 42, 108, 19, 981, 214, 426, 598, 563, 351, 60, 974, 650, 366, 668, 263, 129, 59, 149, 622, 922, 193, 837, 805, 687, 327, 755, 812, 490, 230, 336, 66, 4, 461, 908, 739, 206, 631, 684, 425, 97, 703, 749, 451, 876, 29, 710, 46, 123, 218, 8, 614, 160, 696, 239, 17, 559, 252, 752, 754, 15, 167, 596, 865, 895, 868, 69, 859, 833, 420, 464, 995, 875, 216, 348, 102, 830, 267, 565, 309, 7, 372, 721, 421, 0, 36, 982, 849, 964, 571, 301, 902, 633, 828, 530, 40, 418, 582, 606, 672, 310, 635, 654, 577, 419, 16, 753, 83, 107, 556, 422, 773, 657, 95, 543, 765, 131, 861, 550, 207, 121, 429, 699, 385, 416, 933, 403, 759, 677, 829, 295, 580, 591, 302, 683, 826, 862, 67, 627, 345, 454, 152, 884, 25, 260, 909, 341, 470, 226, 997, 893, 161, 439, 130, 521, 463, 424, 393, 455, 824, 271, 313, 497, 332, 601, 386, 963, 620, 561, 645, 404, 63, 557, 720, 198, 575, 904, 984, 323, 987, 947, 75, 74, 542, 276, 817, 900, 68, 20, 31]\n",
      "closest Centriod is :  7\n",
      "closest Points_indexes is :  [30887, 53410, 85195, 2038, 42091]\n",
      "closest Points score is :  [array([0.8762298], dtype=float32), array([0.872295], dtype=float32), array([0.8636436], dtype=float32), array([0.8622327], dtype=float32), array([0.86140627], dtype=float32)]\n",
      "100K\tscore\t-5000.0\ttime\t0.27\tRAM\t5.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.load(\"./PATH_DB_100K/data_points.npy\",allow_pickle=True)\n",
    "db = VecDB(file_path = 'PATH_DB_100K', new_db = False)\n",
    "db._build_index(vectors)\n",
    "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**5)\n",
    "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
    "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"100K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "del db\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaad3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b684cdf",
   "metadata": {
    "id": "be7b4632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest Centriod is :  292\n",
      "closest Points_indexes is :  [277801, 572046, 505656, 30887, 652664]\n",
      "closest Points score is :  [array([0.889826], dtype=float32), array([0.8799153], dtype=float32), array([0.877457], dtype=float32), array([0.8762298], dtype=float32), array([0.87601733], dtype=float32)]\n",
      "1M\tscore\t-5000.0\ttime\t1.14\tRAM\t5.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.load(\"./PATH_DB_1M/data_points.npy\",allow_pickle=True)\n",
    "db = VecDB(file_path = 'PATH_DB_1M', new_db = False)\n",
    "db._build_index(vectors)\n",
    "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6)\n",
    "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
    "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"1M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "del db\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cba284",
   "metadata": {
    "id": "e9cba284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest Centriod is :  472\n",
      "closest Points_indexes is :  [2354940, 4520447, 3130298, 884893, 2444666]\n",
      "closest Points score is :  [array([0.87778115], dtype=float32), array([0.8748292], dtype=float32), array([0.87368125], dtype=float32), array([0.871892], dtype=float32), array([0.87081224], dtype=float32)]\n",
      "5M\tscore\t-5000.0\ttime\t0.90\tRAM\t5.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.load(\"./PATH_DB_5M/data_points.npy\",allow_pickle=True)\n",
    "db = VecDB(file_path = 'PATH_DB_5M', new_db = False)\n",
    "db._build_index(vectors)\n",
    "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*5)\n",
    "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
    "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"5M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "del db\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e264715d",
   "metadata": {
    "id": "0e67454a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest Centriod is :  621\n",
      "closest Points_indexes is :  [436098, 7623530, 8802913, 8536416, 1998541]\n",
      "closest Points score is :  [array([0.88028765], dtype=float32), array([0.8796703], dtype=float32), array([0.8786013], dtype=float32), array([0.87832755], dtype=float32), array([0.8768755], dtype=float32)]\n",
      "10M\tscore\t-5000.0\ttime\t3.88\tRAM\t5.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.load(\"./PATH_DB_10M/data_points.npy\",allow_pickle=True)\n",
    "db = VecDB(file_path = 'PATH_DB_10M', new_db = False)\n",
    "db._build_index(vectors)\n",
    "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*10)\n",
    "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
    "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"10M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "del db\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ead671",
   "metadata": {
    "id": "d5ead671"
   },
   "outputs": [],
   "source": [
    "vectors = np.load(\"./PATH_DB_15M/data_points.npy\",allow_pickle=True)\n",
    "db = VecDB(file_path = 'PATH_DB_15M', new_db = False)\n",
    "db._build_index(vectors)\n",
    "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*15)\n",
    "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
    "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"15M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "del db\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4565f3",
   "metadata": {
    "id": "33f0e26d"
   },
   "outputs": [],
   "source": [
    "vectors = np.load(\"./PATH_DB_20M/data_points.npy\",allow_pickle=True)\n",
    "db = VecDB(file_path = 'PATH_DB_20M', new_db = False)\n",
    "db._build_index(vectors)\n",
    "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6*20)\n",
    "res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
    "res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
    "eval = evaluate_result(res)\n",
    "to_print = f\"20M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
    "to_print_arr.append(to_print)\n",
    "print(to_print)\n",
    "del db\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1f075",
   "metadata": {
    "id": "03b1f075"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5a5d3",
   "metadata": {
    "id": "25645083"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e4d37",
   "metadata": {
    "id": "ea6e4d37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea924f7f",
   "metadata": {
    "id": "ea924f7f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b1fb6",
   "metadata": {
    "id": "a00b1fb6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00bc0f72",
   "metadata": {
    "id": "00bc0f72"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecd42f",
   "metadata": {
    "id": "e8ecd42f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25d21145",
   "metadata": {
    "id": "25d21145"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b6669",
   "metadata": {
    "id": "b37b6669"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4420a0c7",
   "metadata": {
    "id": "4420a0c7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa4935",
   "metadata": {
    "id": "97aa4935"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d199e23a",
   "metadata": {
    "id": "d199e23a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3e645",
   "metadata": {
    "id": "0ba3e645"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c23262",
   "metadata": {
    "id": "60c23262"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d7556",
   "metadata": {
    "id": "a08d7556"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10eec2",
   "metadata": {
    "id": "4d10eec2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ba40e",
   "metadata": {
    "id": "6c2ba40e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c888167",
   "metadata": {
    "id": "5c888167"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48c69d50",
   "metadata": {
    "id": "48c69d50"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74356f27",
   "metadata": {
    "id": "74356f27"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3d102",
   "metadata": {
    "id": "e8c3d102"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e370b33",
   "metadata": {
    "id": "9e370b33"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c2514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97ee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
